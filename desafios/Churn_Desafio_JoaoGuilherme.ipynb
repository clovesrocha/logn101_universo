1. Definição do Problema/Objetivos (Observatório de Dados)

 Ação: Descreva em um parágrafo o que é o problema de Churn e qual o objetivo principal do projeto de Machine Learning (Ex: Minimizar Falsos Negativos é crucial aqui, por que?).

R:O problema de Churn ocorre quando clientes deixam de usar os produtos ou serviços de uma empresa, o que representa perda de receita e aumento nos custos de aquisição de novos clientes.
Em um projeto de Machine Learning voltado para prever o churn, o objetivo principal é identificar, com antecedência, quais clientes têm maior probabilidade de cancelar, permitindo que a empresa tome ações preventivas (como ofertas, suporte personalizado ou campanhas de retenção).
Minimizar falsos negativos é crucial nesse contexto, pois um falso negativo significa classificar um cliente que realmente vai sair como se fosse permanecer — e isso impede a empresa de agir a tempo para reter esse cliente, resultando em perda direta de faturamento.

2. Coleta e Entendimento dos Dados (Bases alvos)

import pandas as PD

df = pd.read_csv('caminho_do_arquivo.csv')

print(" Primeiras 10 linhas:")
print(df.head(10))

print("\n Tipos de dados (dtypes):")
print(df.dtypes)

print("\n Quantidade de valores ausentes por coluna:")
print(df.isnull().sum())

3. Tratamento/Preparação dos Dados (Pré-Processamento)

a) R: import pandas as pd

df = pd.read_csv('caminho_do_arquivo.csv')

for coluna in df.columns:

    if df[coluna].isnull().sum() > 0:
        print(f" Coluna '{coluna}' possui {df[coluna].isnull().sum()} valores nulos.")

        if df[coluna].dtype in ['float64', 'int64']:
            media = df[coluna].mean()
            df[coluna].fillna(media, inplace=True)
            print(f" Valores nulos da coluna '{coluna}' foram substituídos pela média ({media:.2f}).")

        else:
            moda = df[coluna].mode()[0]
            df[coluna].fillna(moda, inplace=True)
            print(f" Valores nulos da coluna '{coluna}' foram substituídos pela moda ('{moda}').")

print("\n Tratamento de nulos concluído!")

b) R: import pandas as pd

df = pd.read_csv('caminho_do_arquivo.csv')

colunas_categoricas = df.select_dtypes(include=['object', 'category']).columns

print(" Colunas categóricas encontradas:")
print(colunas_categoricas)

df_encoded = pd.get_dummies(df, columns=colunas_categoricas, drop_first=False)

print("\n One-Hot Encoding aplicado com sucesso!")
print(df_encoded.head())

c) R: import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

df = pd.read_csv('caminho_do_arquivo.csv')

colunas_numericas = df.select_dtypes(include=['int64', 'float64']).columns

print(" Colunas numéricas identificadas:")
print(colunas_numericas)

scaler = MinMaxScaler()
df_normalizado = df.copy()
df_normalizado[colunas_numericas] = scaler.fit_transform(df[colunas_numericas])

print("\n Escalonamento aplicado com sucesso!")
print(df_normalizado.head())

4. Análise Exploratória (Mineração)

 R : import pandas as pd

df = pd.read_csv('caminho_do_arquivo.csv')

proporcao = df['Churn'].value_counts(normalize=True)

print(" Proporção de classes (Churn vs Não-Churn):")
print((proporcao * 100).round(2).astype(str) + '%')

5. Modelagem do Algoritmo (Mineração)

R: Ação
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df = pd.read_csv('caminho_do_arquivo.csv')

X = df.drop('Churn', axis=1)
y = df['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

modelo = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
modelo.fit(X_train, y_train)

y_pred = modelo.predict(X_test)

print(" Acurácia:", accuracy_score(y_test, y_pred))
print("\n Relatório de Classificação:")
print(classification_report(y_test, y_pred))
print("\n Matriz de Confusão:")
print(confusion_matrix(y_test, y_pred))

Foco em Lógica e Algoritmos:

R: Para Regressão Logística = A Regressão Logística combina as variáveis em um valor z e a função sigmoide transforma
z em uma probabilidade entre 0 e 1, indicando a chance de um cliente dar churn.

  Para Árvores de Decisão = A Árvore de Decisão escolhe a feature que melhor separa as classes,
  usando Gini ou ganho de informação para medir qual divisão deixa os grupos mais puros.

  6. Análise dos Resultados e Otimização do Modelo (tuning)

  R: from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matri

acuracia = accuracy_score(y_test, y_pred)
print(" Acurácia:", round(acuracia, 4)

precisao = precision_score(y_test, y_pred)
print(" Precisão:", round(precisao, 4))

recall = recall_score(y_test, y_pred)
print(" Recall:", round(recall, 4))

f1 = f1_score(y_test, y_pred)
print(" F1-Score:", round(f1, 4))

print("\n Relatório de Classificação:")
print(classification_report(y_test, y_pred))

print("\n Matriz de Confusão:")
print(confusion_matrix(y_test, y_pred))

      Foco em Lógica e Algoritmos:

      a) Priorize Recall, porque o objetivo é identificar o máximo de clientes que podem cancelar, garantindo que ações de retenção atinjam quem realmente está em risco.

      b)

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
import numpy as np

modelo = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)

scores = cross_val_score(modelo, X, y, cv=5, scoring='recall')  # aqui priorizamos Recall

print(" Recall médio:", round(np.mean(scores), 4))
print(" Desvio padrão:", round(np.std(scores), 4))

A Cross-Validation testa o modelo em diferentes subconjuntos do dataset, g
arantindo que ele generalize melhor e que as métrica não dependam de uma única divisão treino-teste.
